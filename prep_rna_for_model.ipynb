{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef012f38-1548-4b74-9cf3-4d3a987dff4e",
   "metadata": {},
   "source": [
    "## Functions for prepping raw RNA sequence data for DNABERT-2-117M\n",
    "raw RNA -> prep_sequence -> (optionally) make_windows -> tokenize_dnabert2 -> model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b170155a-a15a-4585-ab0c-9a8a586785c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re \n",
    "from typing import Iterable, List, Dict, Optional, Any\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# load tokenizer/model \n",
    "tok = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ef4d5e3-b42b-4779-9835-224634b62877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to prep one sequence\n",
    "\n",
    "def prep_sequence(seq: str) -> str:\n",
    "    \"\"\"Clean an RNA string so it's safe for models. Removes whitespace, uppercases all, U->T, map non-ACGTN to N.\n",
    "    \n",
    "    >>> prep_sequence(\"aau uug-cag\\n\")\n",
    "    'AATTTGCAG'\n",
    "    >>> prep_sequence(\"AAUXX\")\n",
    "    'AATNN'\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if seq is None:\n",
    "        return \"\"\n",
    "    s = \"\".join(str(seq).split()).upper()   \n",
    "    s = s.replace(\"U\", \"T\")                 \n",
    "    s = re.compile(r\"[^ACGTN]\").sub(\"N\", s)\n",
    "    return s\n",
    "\n",
    "def kmers(seq: str, k: int = 6, pad_char: str = \"N\") -> List[str]:\n",
    "    \"\"\" Manual tokenizer for if DNABERT-2 fails. Breaks sequence into substrings of length k, since DNABERT-style models \n",
    "    read DNA as sliding windows of k bases. Adds N for padding if sequence has less than k nucleotides\n",
    "\n",
    "    >>> kmers(\"AAAUUUG\")\n",
    "    ['AAATTT', 'AATTTG']\n",
    "    >>> kmers(\"A\")\n",
    "    ['ANNNNN']\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    s = prep_sequence(seq)\n",
    "    n = len(s)\n",
    "    if n < k:\n",
    "        s = s + pad_char * (k - n)\n",
    "        return [s]\n",
    "    return [s[i:i+k] for i in range(n - k + 1)]\n",
    "\n",
    "def tokenize_dnabert2(seq: str, tokenizer, max_length: int = 300, k: int = 6) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\" Turn a sequence into model-ready tensors of form (input_ids, attention_mask). Tries the DNABERT-2 tokenizer \n",
    "    directly (it usually does k-merization under the hood when loaded with `trust_remote_code=True`). If that fails, \n",
    "    it falls back to manual k-mers joined by spaces.\n",
    "    \n",
    "    >>> enc = tokenize_dnabert2(\"AAAUUUGCAG\", tok, max_length=64)\n",
    "    >>> enc[\"input_ids\"].shape  \n",
    "    torch.Size([1, 64])\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    s = prep_sequence(seq)\n",
    "    try:\n",
    "        return tokenizer(s, add_special_tokens=True, padding=\"max_length\", truncation=True, max_length=max_length,\n",
    "            return_tensors=\"pt\", return_token_type_ids=False)\n",
    "    except Exception:\n",
    "        toks = \" \".join(kmers(s, k))\n",
    "        return tokenizer(toks, add_special_tokens=True, padding=\"max_length\", truncation=True, max_length=max_length,\n",
    "            return_tensors=\"pt\", return_token_type_ids=False)\n",
    "\n",
    "def make_windows(seq: str, window_nt: int = 300, stride: int = 100, center_index: Optional[int] = None) -> List[str]:\n",
    "    \"\"\"Slice a (possibly long) sequence into fixed-length nucleotide windows.\n",
    "\n",
    "    DNABERT-style models run on fixed-size inputs, so for APA tasks we want either one window centered on a \n",
    "    candidate site or sliding windows that scan the whole sequence.\n",
    "\n",
    "    >>> make_windows(\"A\"*20, window_nt=8, stride=4)\n",
    "    ['AAAAAAAA', 'AAAAAAAA', 'AAAAAAAA', 'AAAAAAAA']\n",
    "    \n",
    "    \"\"\"\n",
    "    s = prep_sequence(seq)\n",
    "    n = len(s)\n",
    "    if n == 0:\n",
    "        return []\n",
    "\n",
    "    if center_index is not None:\n",
    "        c = max(0, min(n - 1, int(center_index)))   \n",
    "        half = window_nt // 2\n",
    "        left = max(0, c - half)\n",
    "        right = min(n, left + window_nt)\n",
    "        if right - left < window_nt and left > 0:\n",
    "            left = max(0, right - window_nt)\n",
    "        return [s[left:right]]\n",
    "\n",
    "    if n <= window_nt:\n",
    "        return [s]\n",
    "    return [s[i:i+window_nt] for i in range(0, n - window_nt + 1, stride)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "429e0357-6fb9-4be2-b274-e56675c61498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale up to work on lots of raw sequences at once\n",
    "\n",
    "class APADataset(Dataset):\n",
    "    \"\"\" PyTorch Dataset that:\n",
    "      1) expands each raw sequence into one or more windows (via make_windows),\n",
    "      2) tokenizes each window for DNABERT-2 (via tokenize_dnabert2),\n",
    "      3) stores tensors + lightweight metadata for DataLoader batching.\n",
    "\n",
    "    Each original sequence may yield multiple \"samples\" (one per window).\n",
    "\n",
    "    Shapes \n",
    "    -----\n",
    "    - B: batch size (how many windowed, tokenized sequences)\n",
    "    - L: tokens per windowed, tokenized sequences after padding\n",
    "    \n",
    "    Items\n",
    "    -----\n",
    "    __getitem__(idx) returns a dict with:\n",
    "        - 'input_ids'      : LongTensor[L]\n",
    "        - 'attention_mask' : LongTensor[L]\n",
    "        - 'meta'           : dict with {'seq_index', 'win_index', 'len_nt'}\n",
    "        - 'labels'         : LongTensor[] (only if labels were provided)\n",
    "\n",
    "    Example\n",
    "    -----\n",
    "    >>> seqs = [\"AAAUUUGCAG\", \"ccauuuaaaGG\"]\n",
    "    >>> ds = APADataset(seqs, tok, window_nt=8, stride=4, max_length=64)\n",
    "    >>> len(ds)                        \n",
    "    4\n",
    "    >>> sample = ds[0]\n",
    "    >>> sorted(sample.keys())\n",
    "    ['attention_mask', 'input_ids', 'meta']\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, sequences: Iterable[str], tokenizer, labels: Optional[Iterable[int]] = None, \n",
    "                 centers: Optional[Iterable[Optional[int]]] = None, window_nt: int = 300, stride: int = 100, max_length: int = 300, k: int = 6):\n",
    "        self.samples = []\n",
    "        seq_list = list(sequences)\n",
    "        lab_list = list(labels) if labels is not None else [None]*len(seq_list)\n",
    "        cen_list = list(centers) if centers is not None else [None]*len(seq_list)\n",
    "\n",
    "        for i, (seq, lab, cen) in enumerate(zip(seq_list, lab_list, cen_list)):\n",
    "            for j, w in enumerate(make_windows(seq, window_nt=window_nt, stride=stride, center_index=cen)):\n",
    "                enc = tokenize_dnabert2(w, tokenizer, max_length=max_length, k=k)\n",
    "                item = {\"input_ids\": enc[\"input_ids\"].squeeze(0), \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "                        \"meta\": {\"seq_index\": i, \"win_index\": j, \"len_nt\": len(w)}}\n",
    "                if lab is not None:\n",
    "                    item[\"labels\"] = torch.tensor(int(lab), dtype=torch.long)\n",
    "                self.samples.append(item)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # number of windowed samples\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        # return one windowed and tokenized sample\n",
    "        return self.samples[idx]\n",
    "\n",
    "def apa_collate(batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Collate a list of APADataset items into a single batch for the model.\n",
    "      - Stacks 'input_ids' and 'attention_mask' into (B, L) tensors.\n",
    "      - Preserves 'meta' as a Python list (useful for mapping predictions back).\n",
    "      - If any item has 'labels', stacks them into shape (B,)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    batch : List[Dict]\n",
    "        What DataLoader hands us (a list of items from APADataset.__getitem__).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        {\n",
    "          'input_ids'     : LongTensor[B, L],\n",
    "          'attention_mask': LongTensor[B, L],\n",
    "          'labels'        : LongTensor[B]   (only if present),\n",
    "          'meta'          : List[dict]\n",
    "        }\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> seqs = [\"AAAUUUGCAGUAAUGA\", \"ccauuuaaaGG\"]\n",
    "    >>> ds = APADataset(seqs, tok, window_nt=12, stride=6, max_length=64)\n",
    "    >>> loader = DataLoader(ds, batch_size=2, shuffle=False, collate_fn=apa_collate)\n",
    "    >>> batch = next(iter(loader))\n",
    "    >>> batch[\"input_ids\"].shape  \n",
    "    torch.Size([2, 64])\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    out = {\"input_ids\": torch.stack([b[\"input_ids\"] for b in batch]), \"attention_mask\": torch.stack([b[\"attention_mask\"] for b in batch]),\n",
    "        \"meta\": [b[\"meta\"] for b in batch]}\n",
    "    if any(\"labels\" in b for b in batch):\n",
    "        out[\"labels\"] = torch.stack([b[\"labels\"] for b in batch if \"labels\" in b])\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bf9637-a6e1-4792-94e1-bcf615476530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
